{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to build a network?\n",
    "- Social network simulation?\n",
    "\n",
    "- We have:\n",
    "    - Datetime\n",
    "    - Innovation\n",
    "    - Location\n",
    "- How to build a Network from this?\n",
    "- -> Tree shaped?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV Import\n",
    "- Dask only import if one of the Inno Columns is ==True; \n",
    "- Loop trough all CSV's and append \"tures\" to a single dd\n",
    "- Turn into gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR 1: PROJ: proj_create_from_database: Open of /home/linusrg/Code/LILI/.micromamba/envs/LILI-Env/share/proj failed\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import os\n",
    "# Treat '' and np.inf as NaN\n",
    "pd.options.mode.use_inf_as_na = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '~/Code/LILI/data/' \n",
    "cols = [\"inno_01\",\"inno_02\",\"inno_03\",\"inno_04\",\"inno_05\",\"inno_06\",\"inno_07\",\"inno_08\",\"inno_09\",\"inno_10\",\"inno_12\",\"inno_13\",\"inno_15\",\"inno_16\",\"inno_17\",\"inno_18\",\"inno_19\",\"inno_20\",\"inno_21\",\"inno_22\",\"inno_23\",\"inno_24\",\"inno_25\",\"inno_26\",\"inno_27\",\"inno_29\",\"inno_30\",\"inno_31\",\"inno_32\",\"inno_33\",\"inno_34\",\"inno_39\"]\n",
    "imp_cols = [\"created_at\", \"twitter_id\",\"user_id\",\"user_location\",\"longitude\",\"latitude\",\"place_box_1_long\",\"place_box_1_lat\",\"place_box_2_long\",\"place_box_2_lat\",\"country\",\"country_code\",\"place_full_name\",\"place_id\",\"place_name\"] + cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes={'twitter_id': 'int64',\n",
    "        'created_at': 'object',\n",
    "        'user_id': 'int64',\n",
    "        'user_location': 'string',\n",
    "        'longitude': 'float',\n",
    "        'latitude': 'float',\n",
    "        'place_box_1_long': 'string',\n",
    "        'place_box_1_lat': 'string',\n",
    "        'place_box_2_long': 'string',\n",
    "        'place_box_2_lat': 'string',\n",
    "        'country': 'string',\n",
    "        'country_code': 'string',\n",
    "        'place_full_name': 'string',\n",
    "        'place_id': 'string',\n",
    "        'place_name': 'string',\n",
    "        'place_type': 'string',\n",
    "        'inno_01': 'boolean',\n",
    "        'inno_02': 'boolean',\n",
    "        'inno_03': 'boolean',\n",
    "        'inno_04': 'boolean',\n",
    "        'inno_05': 'boolean',\n",
    "        'inno_06': 'boolean',\n",
    "        'inno_07': 'boolean',\n",
    "        'inno_08': 'boolean',\n",
    "        'inno_09': 'boolean',\n",
    "        'inno_10': 'boolean',\n",
    "        'inno_12': 'boolean',\n",
    "        'inno_13': 'boolean',\n",
    "        'inno_15': 'boolean',\n",
    "        'inno_16': 'boolean',\n",
    "        'inno_17': 'boolean',\n",
    "        'inno_18': 'boolean',\n",
    "        'inno_19': 'boolean',\n",
    "        'inno_20': 'boolean',\n",
    "        'inno_21': 'boolean',\n",
    "        'inno_22': 'boolean',\n",
    "        'inno_23': 'boolean',\n",
    "        'inno_24': 'boolean',\n",
    "        'inno_25': 'boolean',\n",
    "        'inno_26': 'boolean',\n",
    "        'inno_27': 'boolean',\n",
    "        'inno_29': 'boolean',\n",
    "        'inno_30': 'boolean',\n",
    "        'inno_31': 'boolean',\n",
    "        'inno_32': 'boolean',\n",
    "        'inno_33': 'boolean',\n",
    "        'inno_34': 'boolean',\n",
    "        'inno_39': 'boolean'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DaskImp(path, imp_cols=imp_cols):\n",
    "        df = dd.read_csv(path, \n",
    "                sep = '\\t', \n",
    "                lineterminator='\\n', \n",
    "                usecols=imp_cols, \n",
    "                false_values =['', ' ', \"''\", \"' '\", \"\", \" \", 'False', 'Fal', \"['', '', '']\" ], #There is some weired stuff in that Data! Turning it into false Values. \n",
    "                dtype= dtypes\n",
    "                )\n",
    "        return df        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack all the bool to one named column\n",
    "def BoolStack(df_s):\n",
    "    df_s['inno_nr'] = df_s[cols].idxmax(1).astype('string')\n",
    "    df_n = df_s.drop(cols, axis=1)\n",
    "    df_n['inno_nr'] = df_n['inno_nr'].astype('string')\n",
    "    return(df_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create files in subfolders list\n",
    "subfolders = []\n",
    "for root, dirs, files in os.walk('../data'):\n",
    "   for name in files:\n",
    "      if root in subfolders:\n",
    "         pass\n",
    "      else:\n",
    "         subfolders.append(root) \n",
    "subfolders.sort()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create empty csv with actual headers to initialize dask data frame\n",
    "# The empty.csv is included in the repo (unlike the data folder)\n",
    "#df_s = df_i.head(0)\n",
    "#df_s.to_csv('empty.csv', sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop trough folders, parallel import in folders (parallelization across subfolders not possible with Dask, takes ~5s)\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "#ignore the warning, append still works and concat is not implemented in dask yet :)\n",
    "\n",
    "#Create empty header df to append to\n",
    "df_s = BoolStack(DaskImp('empty.csv'))\n",
    "\n",
    "#Loop trough folders\n",
    "for folder in subfolders:\n",
    "     # Import a CSV, stack the Boolean Innovation Columns to one Column\n",
    "     df = BoolStack(DaskImp(f'{folder}/*.csv'))\n",
    "     # Append to the previous, but only columns with an innovation \n",
    "     df_s = df_s.append(df[df.inno_nr != 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Columns:  16\n",
      "Number of Columns:  16\n"
     ]
    }
   ],
   "source": [
    "print('Number of Columns: ', len(df_s.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Delayed('int-cb205802-6d8a-4949-b1e8-6ffc5e64c4c9'), 16)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn dask data frame into pandas data frame\n",
    "#and export this as parquet\n",
    "df = df_s.compute()\n",
    "df.to_parquet('geocoding_all_parallel_0425.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get coordinates for Tweets (Geocoding)\n",
    "- Define one spatial identifier for every user (Coordinate, Place, User Location)\n",
    "- Safe user and place in a separate list\n",
    "- \n",
    "- Geocode each unique location\n",
    "\n",
    "- Slight noise for tweets at same city?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize\n",
    "--> Make a time based visualization for each Innovation\n",
    "\n",
    "- Kepler.GL?"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d9a0fd26f19acfad023c81a399efe002ffb2594adbc625505a6269bcb569d10d"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('default': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
